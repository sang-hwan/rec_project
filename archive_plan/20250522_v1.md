# 📌 현실적이고 구체적인 단계별 실행 계획

## 📋 단계별 진행 현황

| 단계 | 핵심 작업                             | 상태     |
| ---- | ------------------------------------- | -------- |
| 1    | 트윗 데이터 수집 환경 구축            | ✅ 완료   |
| 2    | 일일 트윗 수집 및 정성 분석 (ChatGPT) | 🔶 진행중 |
| 3    | DB 설계 및 데이터 저장 환경 구축      | 🔲 미완료 |
| 4    | 정량 데이터 수집 및 지표 계산         | 🔲 미완료 |
| 5    | 정량 분석 (ChatGPT)                   | 🔲 미완료 |
| 6    | 종합 분석 보고서 작성                 | 🔲 미완료 |
| 7    | 주기적 점검 및 시스템 개선            | 🔲 미완료 |

---

## ✅ **단계별 세부 작업 내용**

### ✅ 1단계: 데이터 수집 환경 구축 (완료)

- **트위터 데이터 수집 환경 구축 완료**
  - `twscrape` 설치 및 계정 로그인 관리 자동화 구현
  - **계정 로그인 관리 고도화**:  
   - 기존 DB 계정과 JSON 계정의 쿠키를 비교·동기화  
   - 신규 계정만 자동 등록
  - 트윗 데이터 수집 Python 스크립트(`collect_tweets.py`) 개발 완료
  - JSON 형식의 데이터 저장 및 미디어 콘텐츠 자동 분류 및 설명 기능 구현
    - 중복 제거·리트윗 처리  
    - 미디어 URL 추출 강화
  - 일일 평균 50~100개의 트윗 수집 완료 (5월 21일 실제 수집 기준)

### 🔶 2단계: 일일 트윗 수집 및 정성 분석 진행 (현재 진행 중)

- 매일 오후 6시 자동 트윗 수집 및 JSON 파일 저장
- 수집된 JSON 데이터를 ChatGPT를 활용하여 분석 중
  - 계정별 트윗 내용을 요약하여 시장 흐름(시황) 및 언급 종목 분석 진행
  - ChatGPT 질의 자동화를 위한 Python 스크립트 구축 및 운용 중
- 실제 분석 결과물을 확보하여 정성 분석의 실효성 검증 중
- 데이터베이스 작업 없이 JSON 파일 기반으로 분석 진행 중

### 🔲 3단계: DB 설계 및 데이터 저장 환경 구축 (예정)

- PostgreSQL 테이블 구조 설계 (tweet_id, username, content, created_at, media_contents 등)
- DB 저장을 위한 별도의 Python 스크립트 개발 예정 (JSON → PostgreSQL 데이터 저장)
- 데이터 조회 성능 최적화를 위한 인덱스 설계 예정 (`created_at`, `username`)

### 🔲 4단계: 정량 데이터 수집 환경 구축 (2~3주 예정)

- OHLCV 및 거시경제 지표 수집 Python 스크립트 작성 예정
- 기술적 지표 계산 자동화 스크립트 개발 예정

### 🔲 5단계: 정량 분석 진행 (매일 또는 주 2~3회 예정)

- 정량 데이터 수집 및 ChatGPT 기반 분석 예정 (약 10~15분 소요 예상)

### 🔲 6단계: 정성·정량 종합 분석 보고서 작성 (주 1~2회 예정)

- 정성 및 정량 분석 결과 결합한 최종 보고서 작성 예정 (약 30분 소요 예상)

### 🔲 7단계: 주기적 점검 및 시스템 개선 (월 1회 예정)

- 분석 품질 평가 및 개선사항 도출 (1~2시간 소요 예상)
- 지속적인 자동화 및 효율성 개선 예정

---

## 🥅 **최종 목표**

> 매일의 데이터 분석을 통해 투자 전략 도출과 안정적 투자 수익 창출

---

## 🔖 **상태값 설명**

- 🔲 **미완료** : 아직 시작 안 한 작업
- 🔶 **진행중** : 현재 진행 중인 작업
- ✅ **완료** : 작업 완료됨
- 🔵 **수정중** : 작업 내용 수정 중
