import os, glob, time
from datetime import datetime, timedelta, timezone
import pandas as pd, yfinance as yf, requests
from dotenv import load_dotenv
from fredapi import Fred
from dbnomics import fetch_series

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”§ ê³µí†µ ìœ í‹¸: DataFrame ìš”ì•½ ì¶œë ¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _summarize_df(df: pd.DataFrame, label: str, rows: int = 3):
    # DataFrame êµ¬ì¡°ì™€ ì¼ë¶€ í–‰ì„ ê°„ë‹¨íˆ print
    try:
        if df is None:
            print(f"[SUMMARY] {label}: <None>")
            return
        if df.empty:
            print(f"[SUMMARY] {label}: <EMPTY> (shape={df.shape})")
            return
        head = df.head(rows).to_string(max_cols=10, max_rows=rows)
        first, last = df.index.min(), df.index.max()
        print(f"[SUMMARY] {label}: shape={df.shape}, index=[{first} â†’ {last}]")
        print(f"[DATA]\n{head}")
    except Exception as e:
        print(f"[WARN] _summarize_df failed for {label}: {e}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ì‚¬ìš©ì ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìì‚°êµ°ë³„ ì¼ë´‰ ohlcv
ASSET_TICKERS = {
    'stocks_etfs': ['TSLA'],   # ì˜ˆ) ['TSLA','AAPL','QQQ']
    'commodities': [],         # ì˜ˆ) ['GC=F','CL=F']
    'forex':       [],         # ì˜ˆ) ['USDKRW=X','EURUSD=X']
    'bonds':       ['DGS10'],  # ì˜ˆ) ['DGS10']
    'crypto':      ['ethereum']
}

# ì£¼ì‹: ì¬ë¬´ì§€í‘œ
# ì½”ì¸: ì˜¨ì²´ì¸ì§€í‘œ
# ê·¸ì™¸ ìì‚°êµ°: ëŒ€ì²´ì§€í‘œê°€ ì—†ìŒ
# ê³µí¬íƒìš•ì§€ìˆ˜ëŠ” ì •ëŸ‰ ë°ì´í„°ì— í¬í•¨ë˜ì–´ì•¼ í•˜ëŠ”ì§€ ì• ë§¤í•¨
QUANT_ASSETS = {
    'TSLA': {
        'type': 'fundamental',
        'ticker': 'TSLA'
    },
    'ethereum': {
        'type': 'onchain',
        'provider': 'coingecko',
        'ticker': 'ethereum'
    },
    # 'fear_greed': {
    #     'type': 'alternative',
    #     'provider': 'custom_api',
    #     'endpoint': 'https://api.alternative.me/fng/?limit=0'
    # }
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ë‚ ì§œ ë²”ìœ„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
now_utc          = datetime.now(timezone.utc)
end_date         = now_utc.date()
start_price_date = end_date - timedelta(days=365 * 5)   # 5ë…„
start_fund_date  = end_date - timedelta(days=365 * 3)   # 3ë…„
start_macro_date = end_date - timedelta(days=365 * 10)  # 10ë…„
start_alt_date   = end_date - timedelta(days=365 * 3)   # 3ë…„

start_price_str = start_price_date.strftime('%Y%m%d')
start_fund_str  = start_fund_date.strftime('%Y%m%d')
start_macro_str = start_macro_date.strftime('%Y%m%d')
start_alt_str   = start_alt_date.strftime('%Y%m%d')
end_str         = end_date.strftime('%Y%m%d')

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. í´ë” ìƒì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR   = os.path.join(os.getcwd(), 'quant_collects')
CATEGORIES = {
    'daily_prices':         os.path.join(BASE_DIR, 'daily_prices'),
    'technical_indicators': os.path.join(BASE_DIR, 'technical_indicators'),
    'fundamentals':         os.path.join(BASE_DIR, 'fundamentals'),
    'alternative_data':     os.path.join(BASE_DIR, 'alternative_data'),
    'macro_raw':            os.path.join(BASE_DIR, 'macroeconomic', 'raw'),
    'macro_processed':      os.path.join(BASE_DIR, 'macroeconomic', 'processed'),
}
print("[STEP] 0. ë””ë ‰í„°ë¦¬ ìƒì„±")
for folder in CATEGORIES.values():
    try:
        os.makedirs(folder, exist_ok=True)
        print(f"[INFO] Directory ready: {folder}")
    except Exception as e:
        print(f"[ERROR] Could not create directory {folder}: {e}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ê³µí†µ ì €ì¥ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_df(df: pd.DataFrame, category: str, filename: str) -> bool:
    folder = CATEGORIES.get(category)
    if not folder:
        print(f"[ERROR] Unknown category: {category}")
        return False

    # â€˜ê°™ì€ ì¢…ëª©Â·ë¼ë²¨â€™ ë°ì´í„° ê°±ì‹  â†’ ì´ì „ íŒŒì¼ ì •ë¦¬
    # prefix: í™•ì¥ì ì œê±° í›„ 'ë ë‚ ì§œ'ë§Œ ì œì™¸ (ex: tsla_short_20200101_20250614.csv)
    base = filename.rsplit('.', 1)[0]
    parts = base.split('_')
    if category == 'technical_indicators':
        prefix = "_".join(parts[:2])
    else:
        prefix = parts[0]
    for old in glob.glob(os.path.join(folder, f"{prefix}_*.csv")):
        try:
            os.remove(old)
            print(f"[DEBUG] Removed old file: {old}")
        except Exception as err:
            print(f"[WARN] Could not remove {old}: {err}")

    try:
        file_path = os.path.join(folder, filename)
        df.to_csv(file_path)
        print(f"[INFO] Saved CSV: {file_path} (shape: {df.shape})")
        return True
    except Exception as err:
        print(f"[ERROR] Failed to save {file_path}: {err}")
        return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5â€‘A. ê°€ê²© ë°ì´í„° ìˆ˜ì§‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_daily_price(ticker: str) -> pd.DataFrame:
    print(f"[STEP] 1â€‘A. Fetching daily price for {ticker}")
    try:
        df = yf.download(
            ticker,
            start=start_price_date,
            end=end_date + timedelta(days=1),
            interval='1d',
            progress=False
        )
        if isinstance(df.columns, pd.MultiIndex):
            df.columns = df.columns.get_level_values(0)
        df.columns = [c.title() for c in df.columns]

        if df.empty:
            print(f"[WARN] No price data for {ticker} (empty DataFrame)")
            return None

        _summarize_df(df.tail(5), f"{ticker} â€‘ raw price (last 5)")

        filename = f"{ticker.lower()}_{start_price_str}_{end_str}.csv"
        if save_df(df, 'daily_prices', filename):
            return df
        return None
    except Exception as err:
        print(f"[ERROR] fetch_daily_price error for {ticker}: {err}")
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5â€‘B. ê¸°ìˆ ì§€í‘œ ê³„ì‚°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PERIODS = {'short': 20, 'mid': 50, 'long': 200}
def compute_technical_indicators(df: pd.DataFrame, ticker: str):
    print(f"[STEP] 1â€‘B. Computing technical indicators for {ticker}")
    try:
        required = {'Open', 'High', 'Low', 'Close', 'Volume'}
        missing  = required.difference(df.columns)
        if missing:
            print(f"[ERROR] Missing columns {missing} for {ticker}")
            return

        weekly = df.resample('W-FRI').agg({
            'Open': 'first', 'High': 'max', 'Low': 'min',
            'Close': 'last', 'Volume': 'sum'
        }).dropna()
        _summarize_df(weekly, f"{ticker} â€‘ weekly (head)")

        data_common = weekly.copy()

        # RSI
        delta = data_common['Close'].diff()
        up    = delta.clip(lower=0)
        down  = -delta.clip(upper=0)
        rs    = up.rolling(14).mean() / down.rolling(14).mean()
        data_common['RSI_14'] = 100 - (100 / (1 + rs))

        # MACD
        ema12 = data_common['Close'].ewm(span=12, adjust=False).mean()
        ema26 = data_common['Close'].ewm(span=26, adjust=False).mean()
        data_common['MACD']        = ema12 - ema26
        data_common['MACD_Signal'] = data_common['MACD'].ewm(span=9, adjust=False).mean()

        # Bollinger Bands
        mavg = data_common['Close'].rolling(20).mean()
        sd   = data_common['Close'].rolling(20).std()
        data_common['BB_Upper'] = mavg + 2 * sd
        data_common['BB_Lower'] = mavg - 2 * sd

        # ATR
        tr = pd.concat([
            data_common['High'] - data_common['Low'],
            (data_common['High'] - data_common['Close'].shift()).abs(),
            (data_common['Low']  - data_common['Close'].shift()).abs()
        ], axis=1).max(axis=1)
        data_common['ATR_14'] = tr.rolling(14).mean()

        # SMA/EMA
        for label, win in PERIODS.items():
            data = data_common.copy()
            data[f"SMA_{win}"] = data['Close'].rolling(win).mean()
            data[f"EMA_{win}"] = data['Close'].ewm(span=win, adjust=False).mean()
            fn = f"{ticker.lower()}_{label}_{start_price_str}_{end_str}.csv"
            save_df(data, 'technical_indicators', fn)

    except Exception as err:
        print(f"[ERROR] compute_technical_indicators error for {ticker}: {err}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5â€‘C. ì±„ê¶Œ ë‹¨ê¸° ì§€í‘œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def compute_bond_indicator(df: pd.DataFrame, ticker: str):
    print(f"[STEP] 1â€‘C. Computing bond MA for {ticker}")
    try:
        col = df.columns[0]
        ma  = df[col].rolling(4).mean().to_frame(name=f"MA4_{col}")
        _summarize_df(ma, f"{ticker} â€‘ MA4 (head)")
        filename = f"{ticker.lower()}_ma4_{start_price_str}_{end_str}.csv"
        save_df(ma, 'technical_indicators', filename)
    except Exception as err:
        print(f"[ERROR] compute_bond_indicator error for {ticker}: {err}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5â€‘D. ì¬ë¬´ì§€í‘œ/ì˜¨ì²´ì¸ì§€í‘œ/ê·¸ì™¸ì§€í‘œ ìˆ˜ì§‘ ë¡œì§
#   Â· type ì— ë”°ë¼ ë‚´ë¶€ helper ë¥¼ í˜¸ì¶œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# --- private helpers ---------------------------------------------------------
def _fetch_fundamental(ticker: str) -> pd.DataFrame | None:
    """Yahoo Finance ì¬ë¬´ì§€í‘œ ë‹¨ê±´ ìŠ¤ëƒ…ìƒ·"""
    try:
        info = yf.Ticker(ticker).info
        df = pd.DataFrame([{
            'Date': now_utc,
            'EPS': info.get('trailingEps'),
            'P/E': info.get('trailingPE'),
            'ROE': info.get('returnOnEquity')
        }]).set_index('Date')
        _summarize_df(df, f"{ticker} â€‘ fundamentals")
        filename = f"{ticker.lower()}_{start_fund_str}_{end_str}.csv"
        save_df(df, 'fundamentals', filename)
        return df
    except Exception as err:
        print(f"[ERROR] _fetch_fundamental error for {ticker}: {err}")
        return None


def _fetch_onchain(provider: str, ticker: str) -> pd.DataFrame | None:
    """Coingecko ë“± ì˜¨ì²´ì¸/ì‹œì¥ ë°ì´í„°"""
    if provider != 'coingecko':
        print(f"[ERROR] Unsupported onâ€‘chain provider: {provider}")
        return None
    try:
        start_ts = int(datetime.combine(start_alt_date, datetime.min.time()).timestamp())
        end_ts   = int(datetime.combine(end_date,       datetime.min.time()).timestamp())
        url = f"https://api.coingecko.com/api/v3/coins/{ticker}/market_chart/range"
        params = {'vs_currency': 'usd', 'from': start_ts, 'to': end_ts}
        for attempt in range(3):
            resp = requests.get(url, params=params, timeout=15)
            if resp.status_code == 429:
                wait = 2 ** attempt
                print(f"[WARN] 429 Too Many Requests (retry in {wait}s)")
                time.sleep(wait)
                continue
            resp.raise_for_status()
            break
        raw = resp.json().get('prices', [])
        if not raw:
            print(f"[WARN] {ticker}: empty list from API")
            return None
        df = pd.DataFrame(raw, columns=['timestamp', 'price'])
        df['Date'] = pd.to_datetime(df['timestamp'], unit='ms')
        df = df.set_index('Date')['price'].resample('W-FRI').last().to_frame()
        _summarize_df(df, f"{ticker} â€‘ weekly onâ€‘chain")
        filename = f"{ticker.lower()}_{start_alt_str}_{end_str}.csv"
        save_df(df, 'alternative_data', filename)
        return df
    except Exception as err:
        print(f"[ERROR] _fetch_onchain error for {ticker}: {err}")
        return None


def _fetch_alternative(endpoint: str) -> pd.DataFrame | None:
    """ì™¸ë¶€ ì»¤ìŠ¤í…€ API ì§€í‘œ ì˜ˆì‹œ (ê°„ë‹¨ ëŒ€ì‘)"""
    try:
        resp = requests.get(endpoint, timeout=15)
        resp.raise_for_status()
        df = pd.json_normalize(resp.json())
        if df.empty:
            print(f"[WARN] alternative: empty DataFrame from {endpoint}")
            return None
        df['Date'] = now_utc
        df = df.set_index('Date')
        _summarize_df(df, f"alternative ({endpoint})")
        filename = f"alt_{start_alt_str}_{end_str}.csv"
        save_df(df, 'alternative_data', filename)
        return df
    except Exception as err:
        print(f"[ERROR] _fetch_alternative error: {err}")
        return None

# --- public dispatcher -------------------------------------------------------
def fetch_metric(asset: str, conf: dict) -> pd.DataFrame | None:
    """í†µí•© ì§€í‘œ ìˆ˜ì§‘ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸"""
    print(f"[STEP] 2. Fetching metric for {asset} (type={conf.get('type')})")
    t = conf.get('type')
    if t == 'fundamental':
        return _fetch_fundamental(conf['ticker'])
    if t == 'onchain':
        return _fetch_onchain(conf['provider'], conf['ticker'])
    if t == 'alternative':
        return _fetch_alternative(conf['endpoint'])
    print(f"[ERROR] Unknown metric type for {asset}: {t}")
    return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5â€‘F. ê±°ì‹œê²½ì œ ë°ì´í„°
#   (ì›ë³¸ ìœ ì§€ â€“ â€˜ê¸°ê°„ ìœ ì§€â€™ëŠ” íŒŒì¼ ì‚­ì œ ë¡œì§ìœ¼ë¡œ ì¶©ì¡±)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def collect_macroeconomic():
    print("[STEP] 4. Starting macroeconomic data collection")
    raw_folder  = CATEGORIES['macro_raw']
    proc_folder = CATEGORIES['macro_processed']
    proc_file   = f"macro_{start_macro_str}_{end_str}.csv"
    proc_path   = os.path.join(proc_folder, proc_file)

    prev_df = None
    try:
        prev_df = pd.read_csv(proc_path, index_col='Date', parse_dates=['Date']) if os.path.exists(proc_path) else None
        if prev_df is not None:
            last_date = prev_df.index.max().date()
            print(f"[TRACE] Existing macro file last date: {last_date}")
        start_eff = start_macro_date
    except Exception as err:
        print(f"[WARN] Could not load existing macro data: {err}")
        start_eff = start_macro_date

    load_dotenv()
    api_key = os.getenv('FRED_API_KEY')
    if not api_key:
        print("[ERROR] FRED_API_KEY is not set in environment (macro skipped)")
        return

    fred = Fred(api_key=api_key)
    indicators = {
        "Industrial_Production":    {"source": "FRED", "fred_code": "INDPRO"},
        "Manufacturing_New_Orders": {"source": "FRED", "fred_code": "AMTMNO"},
        "Manufacturing_Production": {"source": "FRED", "fred_code": "IPMAN"},
        "Manufacturing_Employment": {"source": "FRED", "fred_code": "MANEMP"},
        "Manufacturing_Prices":     {"source": "FRED", "fred_code": "PCUOMFGOMFG"},
        "CPI":                      {"source": "FRED", "fred_code": "CPIAUCSL"},
        "PPI":                      {"source": "FRED", "fred_code": "PPIACO"},
        "PCE":                      {"source": "FRED", "fred_code": "PCEPI"},
        "Inflation_Expectation":    {"source": "FRED", "fred_code": "EXPINF1YR"},
        "Unemployment_Rate":        {"source": "FRED", "fred_code": "UNRATE"},
        "Nonfarm_Payrolls":         {"source": "FRED", "fred_code": "PAYEMS"},
        "Initial_Jobless_Claims":   {"source": "FRED", "fred_code": "ICSA"},
        "Consumer_Confidence":      {"source": "FRED", "fred_code": "UMCSENT"},
        "Retail_Sales":             {"source": "FRED", "fred_code": "RSAFS"},
        "Federal_Funds_Rate":       {"source": "FRED", "fred_code": "FEDFUNDS"},
        "Treasury_10Y":             {"source": "FRED", "fred_code": "DGS10"},
        "Treasury_2Y":              {"source": "FRED", "fred_code": "DGS2"},
        "Yield_Spread":             {"source": "FRED", "fred_code": "T10Y2Y"},
        "Manufacturing_PMI":        {"source": "DBN",  "provider": "ISM", "dataset": "pmi",        "series": "pm"},
        "Services_PMI":             {"source": "DBN",  "provider": "ISM", "dataset": "nm-pmi",    "series": "pm"},
        "Services_New_Orders":      {"source": "DBN",  "provider": "ISM", "dataset": "nm-neword", "series": "in"},
        "Services_Business_Activity":{"source": "DBN", "provider": "ISM", "dataset": "nm-busact", "series": "in"},
    }

    records = []
    for name, info in indicators.items():
        raw_pattern = os.path.join(raw_folder, f"{name.lower()}_*.csv")
        raw_files = glob.glob(raw_pattern)
        if raw_files:
            dates = [datetime.strptime(os.path.basename(f).split('_')[-1].split('.csv')[0], '%Y%m%d').date()
                     for f in raw_files]
            last_raw = max(dates)
            start_eff_i = max(start_eff, last_raw + timedelta(days=1))
        else:
            start_eff_i = start_eff
        print(f"[DEBUG] {name}: effective start date â†’ {start_eff_i}")

        try:
            if info["source"] == "FRED":
                series = fred.get_series(info["fred_code"], start_eff_i, end_date)
                df_raw = series.rename_axis('Date').reset_index(name='Value')
            else:
                df_raw = fetch_series(info["provider"], info["dataset"], info["series"])
                if isinstance(df_raw, pd.Series):
                    df_raw = df_raw.to_frame(name='Value').reset_index()
                df_raw = df_raw.rename(columns={'period': 'Date', 'date': 'Date', 'value': 'Value'})
                df_raw['Date'] = pd.to_datetime(df_raw['Date'])
                df_raw = df_raw[
                    (df_raw['Date'].dt.date >= start_eff_i) &
                    (df_raw['Date'].dt.date <= end_date)
                ]
            if df_raw.empty:
                print(f"[WARN] {name}: no new data after {start_eff_i} (skip)")
                continue

            _summarize_df(df_raw.tail(3), f"{name} RAW (tail)")

            first = df_raw['Date'].dt.date.min().strftime('%Y%m%d')
            last  = df_raw['Date'].dt.date.max().strftime('%Y%m%d')
            raw_name = f"{name.lower()}_{first}_{last}.csv"
            for old in glob.glob(os.path.join(raw_folder, f"{name.lower()}_*.csv")):
                os.remove(old)
            df_raw.to_csv(os.path.join(raw_folder, raw_name), index=False)
            series = df_raw.set_index('Date')['Value'].rename(name)
            if not series.empty:
                records.append(series)

        except Exception as err:
            print(f"[ERROR] Failed macro fetch for {name}: {err}")

    try:
        if not records:
            print("[WARN] No macro records collected (nothing to save)")
            return
        df_all = pd.concat(records, axis=1).resample('ME').last().dropna(how='all')
        if prev_df is not None:
            df_all = pd.concat([prev_df, df_all]).loc[~df_all.index.duplicated(keep='last')]
        _summarize_df(df_all.tail(3), "Macro Processed (tail)")
        save_df(df_all, 'macro_processed', proc_file)
    except Exception as err:
        print(f"[ERROR] Macro processing failed: {err}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. ë©”ì¸ ì‹¤í–‰ë¶€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == '__main__':
    print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print("[START] Data collection pipeline initiated")
    print(f"[INFO] Execution timestamp (UTC): {now_utc}")
    print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")

    try:
        # 1) ì£¼ì‹Â·ETFÂ·ì›ìì¬Â·ì™¸í™˜
        for category in ['stocks_etfs', 'commodities', 'forex']:
            tickers = ASSET_TICKERS.get(category, [])
            print(f"[GROUP] Processing {category} (tickers={tickers})")
            for ticker in tickers:
                df_price = fetch_daily_price(ticker)
                if df_price is not None:
                    compute_technical_indicators(df_price, ticker)
                else:
                    print(f"[INFO] Skip technicalâ€‘indicators for {ticker} (no price data)")

        # 2) ì±„ê¶Œ
        tickers = ASSET_TICKERS.get('bonds', [])
        print(f"[GROUP] Processing bonds (tickers={tickers})")
        for ticker in tickers:
            try:
                load_dotenv()
                fred_api_key = os.getenv("FRED_API_KEY")
                if not fred_api_key:
                    raise ValueError("FRED_API_KEY is not set in .env file.")
                fred = Fred(api_key=fred_api_key)
                series = fred.get_series(ticker, start_price_date, end_date)
                if series.empty:
                    print(f"[WARN] No bond data for {ticker}")
                    continue
                dfb = series.rename("Value").to_frame()
                df_weekly = dfb.resample('W-FRI').last().dropna()
                _summarize_df(df_weekly, f"{ticker} â€‘ weekly bond")
                fn = f"{ticker.lower()}_{start_price_str}_{end_str}.csv"
                save_df(df_weekly, 'daily_prices', fn)
                compute_bond_indicator(df_weekly, ticker)
            except Exception as err:
                print(f"[ERROR] Bond fetch failed for {ticker}: {err}")

        # 3) ì¬ë¬´ì§€í‘œ/ì˜¨ì²´ì¸ì§€í‘œ/ê·¸ì™¸ì§€í‘œ
        if QUANT_ASSETS:
            print(f"[GROUP] Processing QUANT metrics ({list(QUANT_ASSETS.keys())})")
        for asset, conf in QUANT_ASSETS.items():
            fetch_metric(asset, conf)

        # 5) ê±°ì‹œê²½ì œ
        collect_macroeconomic()

    except Exception as err:
        print(f"[FATAL] Unhandled exception in main pipeline: {err}")

    print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print("[END] Data collection complete")
    print("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
